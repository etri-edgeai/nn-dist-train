{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.train_utils import get_data, get_model\n",
    "from models.Update import DatasetSplit\n",
    "from models.test import test_img_local, test_img_local_all_avg\n",
    "\n",
    "import pdb\n",
    "import easydict\n",
    "\n",
    "import sys\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(1)#args.running_idx=args.seed\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in /home/seongyoon/yes/envs/fedfn/lib/python3.7/site-packages (1.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial / Personalized Accuracy(Full)-Shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-----------------------------------------------------\n",
      "local update part: full\n",
      "shard: 10, frac: 0.1, local_ep: 5\n",
      "Before min/max/mean/std of accuracy\n",
      "27.0 72.0 47.23 8.32\n",
      "After min/max/mean/std of accuracy\n",
      "63.0 96.0 82.02 6.04\n",
      "-----------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-----------------------------------------------------\n",
      "local update part: full\n",
      "shard: 50, frac: 0.1, local_ep: 5\n",
      "Before min/max/mean/std of accuracy\n",
      "39.0 62.0 49.72 4.71\n",
      "After min/max/mean/std of accuracy\n",
      "44.0 66.0 54.72 4.96\n",
      "-----------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-----------------------------------------------------\n",
      "local update part: full\n",
      "shard: 100, frac: 0.1, local_ep: 5\n",
      "Before min/max/mean/std of accuracy\n",
      "36.0 65.0 51.33 5.37\n",
      "After min/max/mean/std of accuracy\n",
      "39.0 66.0 52.99 5.37\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = 'mobile' # cnn, mobile\n",
    "dataset = 'cifar100' # cifar10, cifar100 \n",
    "num_classes = 100 # 10, 100\n",
    "momentum = 0.90\n",
    "wd = 1e-05\n",
    "personalization_epoch = 5 # fine-tuning epochs for personalization\n",
    "\n",
    "server_data_ratio = 0.00\n",
    "\n",
    "for shard_per_user in [10, 50, 100]:\n",
    "    for frac in [0.1]:\n",
    "        for local_ep in [5]:\n",
    "            for local_upt_part in ['full']:\n",
    "                args = easydict.EasyDict({'epochs': 320,\n",
    "                                          'num_users': 100,\n",
    "                                          'hetero_option': 'shard',\n",
    "                                          'shard_per_user': shard_per_user,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 50,\n",
    "                                          'bs': 128,\n",
    "                                          'lr': 5e-3,\n",
    "                                          'momentum': momentum,\n",
    "                                          'wd': wd,\n",
    "                                          'lr_decay': 0.1,\n",
    "                                          'model': model,                                          \n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': False,\n",
    "                                          'num_classes': num_classes,                                \n",
    "                                          'gpu': 0,                                          \n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'seed': 0,\n",
    "                                          'fn': True,\n",
    "                                          'verbose': False,\n",
    "                                          'feature_norm' : 1\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "                base_dir = './save/full_and_body/{}_iid{}_num{}_C{}_le{}_m{}_wd{}/shard{}/FedAvg/'.format(\n",
    "                    args.model, args.iid, args.num_users, args.frac, args.local_ep, args.momentum, args.wd, args.shard_per_user)\n",
    "                algo_dir = 'fn_{}/seed_{}/local_upt_{}_lr_0.5'.format(args.fn, args.seed, args.local_upt_part)\n",
    "\n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "                dict_save_path = 'dict_users_100_{}.pkl'.format(args.shard_per_user)\n",
    "                with open(dict_save_path, 'rb') as handle:#기존 pretrained되었을 때 쓰였던 클라이언트 구성으로 덮어씌운다.\n",
    "                    dict_users_train, dict_users_test = pickle.load(handle)\n",
    "\n",
    "                # build model\n",
    "                net_glob = get_model(args)\n",
    "                net_glob.train()\n",
    "\n",
    "                net_local_list = []\n",
    "                for user_ix in range(args.num_users):\n",
    "                    net_local_list.append(copy.deepcopy(net_glob))\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                before_acc_results = []#pretrained 모델에서의 모든 각 클라이언트의 test acc 기록!!\n",
    "                after_acc_results = []\n",
    "                \n",
    "                for user, net_local in enumerate(net_local_list):\n",
    "                    model_save_path = os.path.join(base_dir, algo_dir, 'best_model.pt')#pretrained된 중앙모델 업로드!!\n",
    "                    \n",
    "                    net_local.load_state_dict(torch.load(model_save_path, map_location=args.device), strict=False)\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    before_acc_results.append(acc_test)\n",
    "                    net_local.train()\n",
    "                    ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users_train[user]), batch_size=args.local_bs, shuffle=True)\n",
    "\n",
    "                    body_params = [p for name, p in net_local.named_parameters() if 'linear' not in name]\n",
    "                    head_params = [p for name, p in net_local.named_parameters() if 'linear' in name]\n",
    "                    optimizer = torch.optim.SGD([{'params': body_params, 'lr': args.lr},\n",
    "                                                 {'params': head_params, 'lr': args.lr}],\n",
    "                                                momentum=args.momentum)#full update!!\n",
    "                    \n",
    "                    for iter in range(personalization_epoch):\n",
    "                        for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                            images, labels = images.to(args.device), labels.to(args.device)\n",
    "                            net_local.zero_grad()\n",
    "                            logits = net_local(images)\n",
    "\n",
    "                            loss = criterion(logits, labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    after_acc_results.append(acc_test)#pretrain 이후의 personalized accuracy 기재!!\n",
    "                print (\"-----------------------------------------------------\")\n",
    "                print (\"local update part: {}\".format(local_upt_part))\n",
    "                print (\"shard: {}, frac: {}, local_ep: {}\".format(shard_per_user, frac, local_ep))\n",
    "                print (\"Before min/max/mean/std of accuracy\")\n",
    "                print (np.min(before_acc_results), np.max(before_acc_results), np.mean(before_acc_results), round(np.std(before_acc_results), 2))\n",
    "                print (\"After min/max/mean/std of accuracy\")\n",
    "                print (np.min(after_acc_results), np.max(after_acc_results), np.mean(after_acc_results), round(np.std(after_acc_results), 2))\n",
    "                print (\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial / Personalized Accuracy(Full)-LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-----------------------------------------------------\n",
      "local update part: full\n",
      "LDA: 0.1, frac: 0.1, local_ep: 5\n",
      "Before min/max/mean/std of accuracy\n",
      "27.751196172248804 53.04740406320542 41.30332542022291 5.93\n",
      "After min/max/mean/std of accuracy\n",
      "58.65384615384615 80.75396825396825 69.50300894340913 5.28\n",
      "-----------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-----------------------------------------------------\n",
      "local update part: full\n",
      "LDA: 0.5, frac: 0.1, local_ep: 5\n",
      "Before min/max/mean/std of accuracy\n",
      "39.321357285429144 51.663405088062625 45.730643443517465 3.03\n",
      "After min/max/mean/std of accuracy\n",
      "45.211581291759465 57.95677799607073 51.759796349072296 3.14\n",
      "-----------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-----------------------------------------------------\n",
      "local update part: full\n",
      "LDA: 1.0, frac: 0.1, local_ep: 5\n",
      "Before min/max/mean/std of accuracy\n",
      "38.73873873873874 50.0 44.00169358205927 2.5\n",
      "After min/max/mean/std of accuracy\n",
      "40.765765765765764 54.166666666666664 46.851481326022125 2.62\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = 'mobile' # cnn, mobile\n",
    "dataset = 'cifar100' # cifar10, cifar100 \n",
    "num_classes = 100 # 10, 100\n",
    "momentum = 0.90\n",
    "wd = 1e-05\n",
    "personalization_epoch = 5 # fine-tuning epochs for personalization\n",
    "\n",
    "server_data_ratio = 0.00\n",
    "\n",
    "for alpha in [0.1, 0.5, 1.0]:\n",
    "    for frac in [0.1]:\n",
    "        for local_ep in [5]:\n",
    "            for local_upt_part in ['full']:\n",
    "                args = easydict.EasyDict({'epochs': 320,\n",
    "                                          'num_users': 100,\n",
    "                                          'hetero_option': 'lda',\n",
    "                                          'alpha': alpha,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 50,\n",
    "                                          'bs': 128,\n",
    "                                          'lr': 5e-3,\n",
    "                                          'momentum': momentum,\n",
    "                                          'wd': wd,\n",
    "                                          'lr_decay': 0.1,\n",
    "                                          'model': model,                                          \n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': False,\n",
    "                                          'num_classes': num_classes,                                \n",
    "                                          'gpu': 0,                                          \n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'seed': 0,\n",
    "                                          'fn': True,\n",
    "                                          'verbose': False,\n",
    "                                          'feature_norm' : 1\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "                base_dir = './save/full_and_body/{}_iid{}_num{}_C{}_le{}_m{}_wd{}/alpha{}/FedAvg/'.format(\n",
    "                    args.model, args.iid, args.num_users, args.frac, args.local_ep, args.momentum, args.wd, args.alpha)\n",
    "                algo_dir = 'fn_{}/seed_{}/local_upt_{}_lr_0.5'.format(args.fn, args.seed, args.local_upt_part)\n",
    "\n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "                dict_save_path = 'dict_users_lda_{}_100_pfl.pkl'.format(args.alpha)\n",
    "                with open(dict_save_path, 'rb') as handle:#기존 pretrained되었을 때 쓰였던 클라이언트 구성으로 덮어씌운다.\n",
    "                    dict_users_train, dict_users_test = pickle.load(handle)\n",
    "\n",
    "                # build model\n",
    "                net_glob = get_model(args)\n",
    "                net_glob.train()\n",
    "\n",
    "                net_local_list = []\n",
    "                for user_ix in range(args.num_users):\n",
    "                    net_local_list.append(copy.deepcopy(net_glob))\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                before_acc_results = []#pretrained 모델에서의 모든 각 클라이언트의 test acc 기록!!\n",
    "                after_acc_results = []\n",
    "                \n",
    "                for user, net_local in enumerate(net_local_list):\n",
    "                    model_save_path = os.path.join(base_dir, algo_dir, 'best_model.pt')#pretrained된 중앙모델 업로드!!\n",
    "                    \n",
    "                    net_local.load_state_dict(torch.load(model_save_path, map_location=args.device), strict=False)\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    before_acc_results.append(acc_test)\n",
    "                    net_local.train()\n",
    "                    ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users_train[user]), batch_size=args.local_bs, shuffle=True)\n",
    "\n",
    "                    body_params = [p for name, p in net_local.named_parameters() if 'linear' not in name]\n",
    "                    head_params = [p for name, p in net_local.named_parameters() if 'linear' in name]\n",
    "                    optimizer = torch.optim.SGD([{'params': body_params, 'lr': args.lr},\n",
    "                                                 {'params': head_params, 'lr': args.lr}],\n",
    "                                                momentum=args.momentum)#full update!!\n",
    "                    \n",
    "                    for iter in range(personalization_epoch):\n",
    "                        for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                            images, labels = images.to(args.device), labels.to(args.device)\n",
    "                            net_local.zero_grad()\n",
    "                            logits = net_local(images)\n",
    "\n",
    "                            loss = criterion(logits, labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    after_acc_results.append(acc_test)#pretrain 이후의 personalized accuracy 기재!!\n",
    "                print (\"-----------------------------------------------------\")\n",
    "                print (\"local update part: {}\".format(local_upt_part))\n",
    "                print (\"LDA: {}, frac: {}, local_ep: {}\".format(alpha, frac, local_ep))\n",
    "                print (\"Before min/max/mean/std of accuracy\")\n",
    "                print (np.min(before_acc_results), np.max(before_acc_results), np.mean(before_acc_results), round(np.std(before_acc_results), 2))\n",
    "                print (\"After min/max/mean/std of accuracy\")\n",
    "                print (np.min(after_acc_results), np.max(after_acc_results), np.mean(after_acc_results), round(np.std(after_acc_results), 2))\n",
    "                print (\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedfn",
   "language": "python",
   "name": "fedfn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
