{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56572134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.train_utils import get_data, get_model\n",
    "from models.Update import DatasetSplit\n",
    "from models.test import test_img_local, test_img_local_all, test_img_global\n",
    "\n",
    "import pdb\n",
    "import easydict\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352e5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_net_data_stats(net_dataidx_map, all_targets):\n",
    "    net_cls_counts = {}#각 client가 어떤 label을 몇개씩 가지고 있는지 통계량 기재!!\n",
    "\n",
    "    for net_i, dataidx in net_dataidx_map.items():\n",
    "        unq, unq_cnt = np.unique(all_targets[dataidx], return_counts=True)#전체 train data 중에 net_i번째 client가 가지고 있는 data가 어떤 label을 가지고 있는지의 정보가 unq, unq의 각 element가 몇개 들어있는지 기재하는게 unq_count이다!!\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}#tmp에는 unq가 key unq_count가 value가 되게 기재!!\n",
    "        net_cls_counts[net_i] = tmp\n",
    "    return net_cls_counts #각 client가 어떤 label을 몇개씩 가지고 있는지 통계량 기재!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85296bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Class 1 - Accuracy: 89.10%\n",
      "Success Probability Vector:\n",
      "tensor([9.6876e-01, 1.4279e-03, 1.0120e-02, 2.1006e-03, 1.0723e-03, 1.6486e-04,\n",
      "        7.4577e-04, 8.5857e-04, 8.9297e-03, 5.8231e-03], device='cuda:1')\n",
      "Class 2 - Accuracy: 89.90%\n",
      "Success Probability Vector:\n",
      "tensor([2.5244e-03, 9.8012e-01, 1.2864e-04, 2.9782e-04, 1.9530e-04, 5.1365e-04,\n",
      "        1.0122e-03, 2.9356e-05, 2.1606e-03, 1.3021e-02], device='cuda:1')\n",
      "Class 3 - Accuracy: 74.30%\n",
      "Success Probability Vector:\n",
      "tensor([5.6198e-03, 2.4129e-05, 9.5326e-01, 1.1230e-02, 8.1325e-03, 7.4437e-03,\n",
      "        1.0298e-02, 2.6441e-03, 3.5541e-04, 9.9046e-04], device='cuda:1')\n",
      "Class 4 - Accuracy: 70.30%\n",
      "Success Probability Vector:\n",
      "tensor([2.4079e-03, 7.8181e-04, 8.0868e-03, 9.4512e-01, 4.1113e-03, 2.4570e-02,\n",
      "        9.2278e-03, 2.9696e-03, 1.4315e-03, 1.2913e-03], device='cuda:1')\n",
      "Class 5 - Accuracy: 78.20%\n",
      "Success Probability Vector:\n",
      "tensor([2.4289e-03, 3.6138e-05, 9.1760e-03, 8.3369e-03, 9.6161e-01, 4.0613e-03,\n",
      "        5.2049e-03, 8.1038e-03, 9.5834e-04, 7.9403e-05], device='cuda:1')\n",
      "Class 6 - Accuracy: 65.80%\n",
      "Success Probability Vector:\n",
      "tensor([5.2597e-04, 2.5922e-04, 3.7969e-03, 3.0050e-02, 3.2093e-03, 9.5452e-01,\n",
      "        1.7561e-03, 5.8401e-03, 2.3748e-05, 2.1233e-05], device='cuda:1')\n",
      "Class 7 - Accuracy: 85.80%\n",
      "Success Probability Vector:\n",
      "tensor([1.3489e-03, 3.7915e-04, 4.5767e-03, 1.1259e-02, 4.7596e-03, 1.0277e-03,\n",
      "        9.7531e-01, 1.2589e-04, 4.9318e-04, 7.2087e-04], device='cuda:1')\n",
      "Class 8 - Accuracy: 84.90%\n",
      "Success Probability Vector:\n",
      "tensor([2.3905e-03, 1.0027e-04, 2.1845e-03, 4.2900e-03, 7.4849e-03, 9.0595e-03,\n",
      "        3.2283e-04, 9.7227e-01, 4.5176e-04, 1.4477e-03], device='cuda:1')\n",
      "Class 9 - Accuracy: 89.00%\n",
      "Success Probability Vector:\n",
      "tensor([6.9659e-03, 1.9598e-03, 1.2161e-03, 1.5363e-03, 1.4684e-04, 1.0355e-04,\n",
      "        1.1289e-03, 1.4610e-04, 9.8291e-01, 3.8902e-03], device='cuda:1')\n",
      "Class 10 - Accuracy: 91.70%\n",
      "Success Probability Vector:\n",
      "tensor([5.0337e-03, 7.3395e-03, 5.9535e-04, 6.8525e-04, 1.3910e-04, 9.6754e-05,\n",
      "        3.9842e-04, 1.1484e-03, 2.8416e-03, 9.8172e-01], device='cuda:1')\n",
      "Class 1 - Accuracy: 10.90%\n",
      "Failure Probability Vector:\n",
      "tensor([0.1085, 0.0624, 0.1591, 0.0654, 0.0637, 0.0131, 0.0440, 0.0504, 0.2277,\n",
      "        0.2056], device='cuda:1')\n",
      "Class 2 - Accuracy: 10.10%\n",
      "Failure Probability Vector:\n",
      "tensor([0.1054, 0.0894, 0.0019, 0.0200, 0.0006, 0.0341, 0.0332, 0.0058, 0.1200,\n",
      "        0.5896], device='cuda:1')\n",
      "Class 3 - Accuracy: 25.70%\n",
      "Failure Probability Vector:\n",
      "tensor([0.1815, 0.0088, 0.0702, 0.1894, 0.2062, 0.0992, 0.1128, 0.0725, 0.0329,\n",
      "        0.0265], device='cuda:1')\n",
      "Class 4 - Accuracy: 29.70%\n",
      "Failure Probability Vector:\n",
      "tensor([0.0738, 0.0121, 0.1276, 0.0953, 0.1473, 0.2730, 0.1415, 0.0697, 0.0333,\n",
      "        0.0264], device='cuda:1')\n",
      "Class 5 - Accuracy: 21.80%\n",
      "Failure Probability Vector:\n",
      "tensor([0.0665, 0.0099, 0.1884, 0.2335, 0.0777, 0.1094, 0.1344, 0.1538, 0.0160,\n",
      "        0.0103], device='cuda:1')\n",
      "Class 6 - Accuracy: 34.20%\n",
      "Failure Probability Vector:\n",
      "tensor([0.0321, 0.0049, 0.0587, 0.5398, 0.1281, 0.0729, 0.0553, 0.0883, 0.0059,\n",
      "        0.0140], device='cuda:1')\n",
      "Class 7 - Accuracy: 14.20%\n",
      "Failure Probability Vector:\n",
      "tensor([0.0454, 0.0064, 0.2232, 0.3716, 0.1609, 0.0759, 0.0686, 0.0144, 0.0193,\n",
      "        0.0142], device='cuda:1')\n",
      "Class 8 - Accuracy: 15.10%\n",
      "Failure Probability Vector:\n",
      "tensor([0.0997, 0.0022, 0.0887, 0.2761, 0.1671, 0.1941, 0.0341, 0.0814, 0.0065,\n",
      "        0.0501], device='cuda:1')\n",
      "Class 9 - Accuracy: 11.00%\n",
      "Failure Probability Vector:\n",
      "tensor([0.4331, 0.1133, 0.0793, 0.0965, 0.0211, 0.0169, 0.0229, 0.0067, 0.0789,\n",
      "        0.1313], device='cuda:1')\n",
      "Class 10 - Accuracy: 8.30%\n",
      "Failure Probability Vector:\n",
      "tensor([0.1517, 0.3056, 0.0332, 0.0990, 0.0036, 0.0144, 0.0714, 0.0541, 0.1845,\n",
      "        0.0823], device='cuda:1')\n",
      "0.819\n"
     ]
    }
   ],
   "source": [
    "model = 'vgg' # cnn, mobile\n",
    "dataset = 'cifar10' # cifar10, cifar100 \n",
    "num_classes = 10 # 10, 100\n",
    "momentum = 0.90\n",
    "wd = 1e-5\n",
    "server_data_ratio=0.0\n",
    "\n",
    "\n",
    "for shard_per_user in [10]:\n",
    "    for frac in [0.1]:\n",
    "        for local_ep in [15]:\n",
    "            for local_upt_part, aggr_part in [('body', 'body')]:\n",
    "                args = easydict.EasyDict({'epochs': local_ep,\n",
    "                                          'num_users': 100,\n",
    "                                          'shard_per_user': shard_per_user,\n",
    "                                          'server_data_ratio': server_data_ratio,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 500,\n",
    "                                          'bs': 50,\n",
    "                                          'lr': 0.01,\n",
    "                                          'momentum': momentum,\n",
    "                                          'wd': wd,\n",
    "                                          'model': model,\n",
    "                                          \n",
    "\n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': False,\n",
    "                                          'num_classes': num_classes,\n",
    "                                          'gpu': 1,\n",
    "                                          'verbose': False,\n",
    "                                          'seed': 1,\n",
    "                                          'test_freq': 1,\n",
    "                                          'load_fed': '',\n",
    "                                          'results_save': 'run1',\n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'aggr_part': aggr_part,\n",
    "                                          'feature_norm': 1,\n",
    "                                          'fn': False,\n",
    "                                          'hetero_option': \"shard\"\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "                base_dir = './save/full_and_body/{}_iid{}_num{}_C{}_le{}_m{}_wd{}_round_320/shard{}/decay_0.1/fn_{}/seed_0/FedAvg'.format(\n",
    "                    args.model, args.iid, args.num_users, args.frac, args.local_ep, args.momentum, args.wd,args.shard_per_user, args.fn)\n",
    "                algo_dir = 'local_upt_{}_lr_{}'.format(args.local_upt_part, args.lr)\n",
    "                \n",
    " \n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "    \n",
    "                test_dataloader = DataLoader(dataset_test, batch_size=args.bs, shuffle=False)\n",
    "   \n",
    "\n",
    "                # build model\n",
    "                model = get_model(args)\n",
    "                model_save_path = os.path.join(base_dir, algo_dir, 'best_model.pt')#pretrained된 중앙모델 업로드!!\n",
    "                model.load_state_dict(torch.load(model_save_path, map_location=args.device), strict=True)\n",
    "                \n",
    "                model.eval()\n",
    "\n",
    "\n",
    "                # 클래스별로 예측 성공한 데이터와 예측 실패한 데이터의 개수를 저장할 리스트를 생성합니다.\n",
    "                success_counts = [0 for _ in range(10)]\n",
    "                failure_counts = [0 for _ in range(10)]\n",
    "\n",
    "                # 클래스별로 softmax 확률을 누적할 리스트를 생성합니다.\n",
    "                success_prob_sums = [torch.zeros(10).to(args.device) for _ in range(10)]\n",
    "                failure_prob_sums = [torch.zeros(10).to(args.device) for _ in range(10)]\n",
    "                \n",
    "                # 클래스별로 정확한 예측 수를 저장할 리스트를 생성합니다.\n",
    "                accuracies = [0 for _ in range(10)]\n",
    "\n",
    "\n",
    "                # 각 데이터의 예측 결과를 확인하고 클래스별로 softmax 확률을 누적합니다.\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in test_dataloader:\n",
    "                        if args.gpu != -1:\n",
    "                            images, labels = images.to(args.device), labels.to(args.device)\n",
    "\n",
    "                        outputs = model(images)\n",
    "                        softmax_probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "                        predicted_labels = torch.argmax(softmax_probs, dim=1)\n",
    "\n",
    "                        for i in range(len(labels)):\n",
    "                            label = labels[i].item()\n",
    "                            predicted_label = predicted_labels[i].item()\n",
    "                            prob_vector = softmax_probs[i]\n",
    "\n",
    "                            if predicted_label == label:\n",
    "                                # 예측 성공한 경우\n",
    "                                success_counts[label] += 1\n",
    "                                success_prob_sums[label] += prob_vector\n",
    "                                accuracies[label] += 1\n",
    "                            else:\n",
    "                                # 예측 실패한 경우\n",
    "                                failure_counts[label] += 1\n",
    "                                failure_prob_sums[label] += prob_vector\n",
    "                                \n",
    "\n",
    "            # 클래스별로 평균 softmax 확률 벡터를 계산합니다.\n",
    "            success_prob_vectors = [success_prob_sums[i] / success_counts[i] for i in range(10)]\n",
    "            failure_prob_vectors = [failure_prob_sums[i] / failure_counts[i] for i in range(10)]\n",
    "            classwise_accuracy=[]\n",
    "\n",
    "            # 결과 출력\n",
    "            for i in range(10):\n",
    "                print(f\"Class {i+1} - Accuracy: {success_counts[i]/(success_counts[i]+failure_counts[i]):.2%}\")\n",
    "                classwise_accuracy.append(success_counts[i]/(success_counts[i]+failure_counts[i]))\n",
    "                print(\"Success Probability Vector:\")\n",
    "                print(success_prob_vectors[i])\n",
    "            \n",
    "            # 결과 출력\n",
    "            for i in range(10):\n",
    "                print(f\"Class {i+1} - Accuracy: {1-success_counts[i]/(success_counts[i]+failure_counts[i]):.2%}\")\n",
    "                \n",
    "                print(\"Failure Probability Vector:\")\n",
    "                print(failure_prob_vectors[i])\n",
    "            print(sum(classwise_accuracy) / len(classwise_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f37b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 (Successful): Mean = 72.17676544189453, Variance = 241.4778289794922, Min = 39.22475814819336, Max = 135.89675903320312\n",
      "Class 1 (Failed): Mean = 63.065738677978516, Variance = 137.9827117919922, Min = 39.89421844482422, Max = 112.2444076538086\n",
      "Class 2 (Successful): Mean = 97.19718933105469, Variance = 879.1378784179688, Min = 43.719444274902344, Max = 215.2157440185547\n",
      "Class 2 (Failed): Mean = 68.46099853515625, Variance = 184.2969207763672, Min = 38.309967041015625, Max = 108.23635864257812\n",
      "Class 3 (Successful): Mean = 74.78334045410156, Variance = 263.01959228515625, Min = 35.88726043701172, Max = 137.8595733642578\n",
      "Class 3 (Failed): Mean = 62.03133773803711, Variance = 99.44964599609375, Min = 38.638580322265625, Max = 92.02568817138672\n",
      "Class 4 (Successful): Mean = 69.04388427734375, Variance = 197.5635528564453, Min = 41.232086181640625, Max = 172.73670959472656\n",
      "Class 4 (Failed): Mean = 62.55174255371094, Variance = 98.93965148925781, Min = 43.102272033691406, Max = 94.94998168945312\n",
      "Class 5 (Successful): Mean = 74.17687225341797, Variance = 268.0448913574219, Min = 39.84779357910156, Max = 152.98243713378906\n",
      "Class 5 (Failed): Mean = 63.26948928833008, Variance = 103.87403106689453, Min = 39.42713165283203, Max = 104.53797912597656\n",
      "Class 6 (Successful): Mean = 71.45356750488281, Variance = 230.1234130859375, Min = 41.35029220581055, Max = 151.52293395996094\n",
      "Class 6 (Failed): Mean = 62.96089172363281, Variance = 120.09130096435547, Min = 41.10547637939453, Max = 117.00484466552734\n",
      "Class 7 (Successful): Mean = 73.96475219726562, Variance = 263.6256103515625, Min = 38.94187927246094, Max = 155.42872619628906\n",
      "Class 7 (Failed): Mean = 60.98750305175781, Variance = 91.44837951660156, Min = 43.65626907348633, Max = 91.2108383178711\n",
      "Class 8 (Successful): Mean = 89.12041473388672, Variance = 772.8175048828125, Min = 40.696346282958984, Max = 208.30789184570312\n",
      "Class 8 (Failed): Mean = 61.49372482299805, Variance = 97.32837677001953, Min = 38.26150131225586, Max = 98.07782745361328\n",
      "Class 9 (Successful): Mean = 81.27745819091797, Variance = 368.49322509765625, Min = 38.530113220214844, Max = 164.24256896972656\n",
      "Class 9 (Failed): Mean = 61.79871368408203, Variance = 104.43424987792969, Min = 42.555294036865234, Max = 98.06548309326172\n",
      "Class 10 (Successful): Mean = 90.6069564819336, Variance = 649.8140869140625, Min = 38.05305862426758, Max = 202.3129119873047\n",
      "Class 10 (Failed): Mean = 65.62316131591797, Variance = 267.0077209472656, Min = 43.213897705078125, Max = 131.66188049316406\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize variables to store results\n",
    "successful_norms = [[] for _ in range(10)]  # List to store successful data feature vector norms for each class\n",
    "failed_norms = [[] for _ in range(10)]  # List to store failed data feature vector norms for each class\n",
    "\n",
    "# Iterate over the test dataset and collect feature vector norms\n",
    "for images, labels in test_dataloader:\n",
    "    images = images.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    norm = torch.norm(model.extract_features(images), dim=1).item()\n",
    "    class_index = labels.item()\n",
    "\n",
    "    if predicted == labels:  # Successful prediction\n",
    "        successful_norms[class_index].append(norm)\n",
    "    else:  # Failed prediction\n",
    "        failed_norms[class_index].append(norm)\n",
    "\n",
    "# Compute mean and variance of feature vector norms for each class\n",
    "successful_means = [torch.tensor(norms).mean().item() for norms in successful_norms]\n",
    "successful_variances = [torch.tensor(norms).var().item() for norms in successful_norms]\n",
    "\n",
    "successful_min = [torch.tensor(norms).min().item() for norms in successful_norms]\n",
    "successful_max = [torch.tensor(norms).max().item() for norms in successful_norms]\n",
    "\n",
    "failed_means = [torch.tensor(norms).mean().item() for norms in failed_norms]\n",
    "failed_variances = [torch.tensor(norms).var().item() for norms in failed_norms]\n",
    "failed_min = [torch.tensor(norms).min().item() for norms in failed_norms]\n",
    "failed_max = [torch.tensor(norms).max().item() for norms in failed_norms]\n",
    "\n",
    "# Print the results\n",
    "for class_index in range(10):\n",
    "    print(f\"Class {class_index + 1} (Successful): Mean = {successful_means[class_index]}, Variance = {successful_variances[class_index]}, Min = {successful_min[class_index]}, Max = {successful_max[class_index]}\")\n",
    "    print(f\"Class {class_index + 1} (Failed): Mean = {failed_means[class_index]}, Variance = {failed_variances[class_index]}, Min = {failed_min[class_index]}, Max = {failed_max[class_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b41107",
   "metadata": {},
   "source": [
    "# Get a feature centroid vector and each vector's norm and cosine similarity each other (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb988604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model = 'vgg' # cnn, mobile\n",
    "dataset = 'cifar10' # cifar10, cifar100 \n",
    "num_classes = 10 # 10, 100\n",
    "momentum = 0.90\n",
    "wd = 1e-5\n",
    "server_data_ratio=0.0\n",
    "\n",
    "\n",
    "for shard_per_user in [10]:\n",
    "    for frac in [0.1]:\n",
    "        for local_ep in [15]:\n",
    "            for local_upt_part, aggr_part in [('body', 'body')]:\n",
    "                args = easydict.EasyDict({'epochs': local_ep,\n",
    "                                          'num_users': 100,\n",
    "                                          'shard_per_user': shard_per_user,\n",
    "                                          'server_data_ratio': server_data_ratio,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 500,\n",
    "                                          'bs': 50,\n",
    "                                          'lr': 0.01,\n",
    "                                          'momentum': momentum,\n",
    "                                          'wd': wd,\n",
    "                                          'model': model,\n",
    "                                          \n",
    "\n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': False,\n",
    "                                          'num_classes': num_classes,\n",
    "                                          'gpu': 1,\n",
    "                                          'verbose': False,\n",
    "                                          'seed': 1,\n",
    "                                          'test_freq': 1,\n",
    "                                          'load_fed': '',\n",
    "                                          'results_save': 'run1',\n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'aggr_part': aggr_part,\n",
    "                                          'feature_norm': 1,\n",
    "                                          'fn': False,\n",
    "                                          'hetero_option': \"shard\"\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "                base_dir = './save/full_and_body/{}_iid{}_num{}_C{}_le{}_m{}_wd{}_round_320/shard{}/decay_0.1/fn_{}/seed_0/FedAvg'.format(\n",
    "                    args.model, args.iid, args.num_users, args.frac, args.local_ep, args.momentum, args.wd,args.shard_per_user, args.fn)\n",
    "                algo_dir = 'local_upt_{}_lr_{}'.format(args.local_upt_part, args.lr)\n",
    "                \n",
    " \n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "    \n",
    "                dict_save_path = 'dict_users_10_{}.pkl'.format(args.shard_per_user)\n",
    "                with open(dict_save_path, 'rb') as handle:#기존 pretrained되었을 때 쓰였던 클라이언트 구성으로 덮어씌운다.\n",
    "                    dict_users_train, dict_users_test = pickle.load(handle)\n",
    "    \n",
    "\n",
    "                # build model\n",
    "                net_glob = get_model(args)\n",
    "                net_glob.train()\n",
    "                \n",
    "\n",
    "                net_local_list = []\n",
    "                for user_ix in range(args.num_users):\n",
    "                    net_local_list.append(copy.deepcopy(net_glob))\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                before_acc_results = []#pretrained 모델에서의 모든 각 클라이언트의 test acc 기록!!\n",
    "                after_acc_results = []\n",
    "                \n",
    "                for user, net_local in enumerate(net_local_list):\n",
    "                    model_save_path = os.path.join(base_dir, algo_dir, 'best_model.pt')#pretrained된 중앙모델 업로드!!\n",
    "                    net_local.load_state_dict(torch.load(model_save_path, map_location=args.device), strict=True)\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    before_acc_results.append(acc_test)\n",
    "                    \n",
    "                print(before_acc_results)\n",
    "\n",
    "                print (\"Before min/max/mean/std of accuracy\")\n",
    "                print (np.min(before_acc_results), np.max(before_acc_results), np.mean(before_acc_results), round(np.std(before_acc_results), 2))\n",
    "                    \n",
    "                    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\">>> Distributing client train data...\")\n",
    "    traindata_cls_dict = record_net_data_stats(dict_users_train, np.array(dataset_train.targets))\n",
    "    print('Data statistics: %s' % str(traindata_cls_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\">>> Distributing client test data...\")\n",
    "    testdata_cls_dict = record_net_data_stats(dict_users_test, np.array(dataset_test.targets))\n",
    "    print('Data statistics: %s' % str(testdata_cls_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f414341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    head_params = [p for name, p in net_local_list[0].named_parameters() if 'classifier' in name]\n",
    "\n",
    "    #Get the innerproduct of the classifier part\n",
    "\n",
    "    print(head_params[0].shape)\n",
    "\n",
    "    print(head_params[1].shape)\n",
    "    print(head_params[1])\n",
    "\n",
    "\n",
    "    # Get the inner product result of the classifier\n",
    "    print(\"Weight norm square of each class part of classifier\")\n",
    "    print(torch.diagonal(torch.mm(head_params[0],head_params[0].transpose(0,1))))\n",
    "\n",
    "\n",
    "    # Get the cosine similarity result of the classifier\n",
    "    print(\"Cosine similarity of each class part of classifier\")\n",
    "\n",
    "    normalized_classifier=nn.functional.normalize(head_params[0], p=2, dim=1)\n",
    "    print(torch.mm(normalized_classifier, normalized_classifier.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Weight norm of each class part of classifier\")\n",
    "print(torch.sqrt(torch.diagonal(torch.mm(head_params[0],head_params[0].transpose(0,1)))))\n",
    "\n",
    "class_index=[i for i in range(num_classes)]\n",
    "\n",
    "plt.bar(class_index, torch.sqrt(torch.diagonal(torch.mm(head_params[0],head_params[0].transpose(0,1)))).cpu().detach().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92607508",
   "metadata": {},
   "source": [
    "# HeatMap of weight bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14648ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 2D array로 변환\n",
    "cos_sim_np = torch.mm(normalized_classifier, normalized_classifier.transpose(0,1)).cpu().detach().numpy()\n",
    "# heatmap 그리기\n",
    "plt.imshow(cos_sim_np, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3f598",
   "metadata": {},
   "source": [
    "# Get a feature vector centroid and each centroid vector's norm and cosine similarity each other (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "                net_local_list[0].eval()\n",
    "                \n",
    "                label= [i for i in range(num_classes)]\n",
    "                \n",
    "                ldr_test = DataLoader(dataset_test, batch_size=args.local_bs, shuffle=False)\n",
    "                class_sums = {i: None for i in label}\n",
    "                class_counts = {i: None for i in label}\n",
    "\n",
    "\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for idx, (images, labels) in enumerate(ldr_test):\n",
    "                        images, labels = images.to(args.device), labels.to(args.device)\n",
    "                        features = net_local_list[0].extract_features(images)\n",
    "\n",
    "                        for i in range(len(label)):\n",
    "                            class_mask = labels == label[i]  \n",
    "\n",
    "                            if class_mask.any():  # 클래스에 속하는 데이터가 있는 경우에만 해당\n",
    "                                class_features = features[class_mask]\n",
    "                                class_sum = class_features.sum(dim=0)\n",
    "                                count=class_features.shape[0]\n",
    "\n",
    "                                if class_sums[label[i]]== None and class_counts[label[i]] == None:\n",
    "                                    class_sums[label[i]]=class_sum\n",
    "                                    class_counts[label[i]]=count\n",
    "                                else:\n",
    "                                    class_sums[label[i]]+=class_sum\n",
    "                                    class_counts[label[i]]+=count\n",
    "\n",
    "                #Get the class-wise feature centroid                    \n",
    "                class_mean_dict={}\n",
    "                for key, value in class_sums.items():\n",
    "                    if key in class_counts:\n",
    "                        class_mean_dict[key] = value / class_counts[key]\n",
    "\n",
    "\n",
    "                #Get a inner product of centroids\n",
    "\n",
    "                cos_sim_hist={}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inner_product_hist={key: 0 for key in class_mean_dict.keys()}\n",
    "                    for images, labels in ldr_test:\n",
    "                        images, labels = images.to(args.device), labels.to(args.device)\n",
    "                        features = net_local_list[0].extract_features(images)\n",
    "                        for i in range(len(label)):\n",
    "\n",
    "                            mean_feature=class_mean_dict[label[i]]\n",
    "\n",
    "                            normalized_mean_feature=mean_feature/torch.norm(mean_feature)\n",
    "\n",
    "                            class_mask = labels == label[i]  # \"cat\" 클래스의 마스크를 생성합니다.\n",
    "\n",
    "                            if class_mask.any():  # 클래스에 속하는 데이터가 있는 경우에만 평균 벡터를 계산합니다.\n",
    "                                class_features = features[class_mask]\n",
    "                                normalized_class_features=nn.functional.normalize(class_features, p=2, dim=1)\n",
    "\n",
    "                                inner_product=torch.mm(normalized_class_features, normalized_mean_feature.unsqueeze(1))\n",
    "                                if label[i] not in cos_sim_hist.keys():\n",
    "                                    if class_features.shape[0]==1:\n",
    "                                        cos_sim_hist[label[i]]=np.array([inner_product.squeeze().cpu().numpy()[()]])\n",
    "                                    else:\n",
    "                                        cos_sim_hist[label[i]]=inner_product.squeeze().cpu().numpy()\n",
    "                                else:\n",
    "                                    if class_features.shape[0]==1:\n",
    "                                        cos_sim_hist[label[i]]=np.concatenate((cos_sim_hist[label[i]], np.array([inner_product.squeeze().cpu().numpy()[()]])), axis=0)\n",
    "\n",
    "                                    else:\n",
    "                                        cos_sim_hist[label[i]]=np.concatenate((cos_sim_hist[label[i]], inner_product.squeeze().cpu().numpy()), axis=0)\n",
    "\n",
    "                                        \n",
    "                cos_sim_mean={}\n",
    "                cos_sim_var={}\n",
    "\n",
    "                for key in class_sums.keys():\n",
    "                    if key in class_counts:\n",
    "                        cos_sim_mean[key] = cos_sim_hist[key].mean()\n",
    "                        cos_sim_var[key] = cos_sim_hist[key].var()\n",
    "                print(\"-------------------MEAN-------------------\")                \n",
    "\n",
    "                print(cos_sim_mean)\n",
    "                print(\"-------------------VAR-------------------\")                \n",
    "\n",
    "                print(cos_sim_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf22bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class_index=[i for i in range(num_classes)]\n",
    "\n",
    "#centroid로부터 cosine similarity 값들의 mean, variance\n",
    "classwise_mean=list(cos_sim_mean.values())\n",
    "classwise_var=list(cos_sim_var.values())\n",
    "\n",
    "plt.errorbar(class_index, classwise_mean, classwise_var, fmt='o', capsize=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe977d",
   "metadata": {},
   "source": [
    "# Get a feature vector centroid and each centroid vector's norm and cosine similarity each other (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a752de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'vgg' # cnn, mobile\n",
    "dataset = 'cifar10' # cifar10, cifar100 \n",
    "num_classes = 10 # 10, 100\n",
    "momentum = 0.90\n",
    "wd = 1e-5\n",
    "server_data_ratio=0.0\n",
    "\n",
    "\n",
    "for shard_per_user in [10]:\n",
    "    for frac in [0.1]:\n",
    "        for local_ep in [15]:\n",
    "            for local_upt_part, aggr_part in [('body', 'body')]:\n",
    "                args = easydict.EasyDict({'epochs': local_ep,\n",
    "                                          'num_users': 100,\n",
    "                                          'shard_per_user': shard_per_user,\n",
    "                                          'server_data_ratio': server_data_ratio,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 500,\n",
    "                                          'bs': 50,\n",
    "                                          'lr': 0.01,\n",
    "                                          'momentum': momentum,\n",
    "                                          'wd': wd,\n",
    "                                          'model': model,\n",
    "                                          \n",
    "\n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': False,\n",
    "                                          'num_classes': num_classes,\n",
    "                                          'gpu': 1,\n",
    "                                          'verbose': False,\n",
    "                                          'seed': 1,\n",
    "                                          'test_freq': 1,\n",
    "                                          'load_fed': '',\n",
    "                                          'results_save': 'run1',\n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'aggr_part': aggr_part,\n",
    "                                          'feature_norm': 1,\n",
    "                                          'fn': False,\n",
    "                                          'hetero_option': \"shard\"\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "                base_dir = './save/full_and_body/{}_iid{}_num{}_C{}_le{}_m{}_wd{}_round_320/shard{}/decay_0.1/fn_{}/seed_0/FedAvg'.format(\n",
    "                    args.model, args.iid, args.num_users, args.frac, args.local_ep, args.momentum, args.wd,args.shard_per_user, args.fn)\n",
    "                algo_dir = 'local_upt_{}_lr_{}'.format(args.local_upt_part, args.lr)\n",
    "                \n",
    " \n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "    \n",
    "                dict_save_path = 'dict_users_10_{}.pkl'.format(args.shard_per_user)\n",
    "                with open(dict_save_path, 'rb') as handle:#기존 pretrained되었을 때 쓰였던 클라이언트 구성으로 덮어씌운다.\n",
    "                    dict_users_train, dict_users_test = pickle.load(handle)\n",
    "    \n",
    "\n",
    "                # build model\n",
    "                net_glob = get_model(args)\n",
    "                net_glob.train()\n",
    "                \n",
    "\n",
    "                net_local_list = []\n",
    "                for user_ix in range(args.num_users):\n",
    "                    net_local_list.append(copy.deepcopy(net_glob))\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                before_acc_results = []#pretrained 모델에서의 모든 각 클라이언트의 test acc 기록!!\n",
    "                after_acc_results = []\n",
    "                \n",
    "                for user, net_local in enumerate(net_local_list):\n",
    "                    model_save_path = os.path.join(base_dir, algo_dir, 'best_model.pt')#pretrained된 중앙모델 업로드!!\n",
    "                    net_local.load_state_dict(torch.load(model_save_path, map_location=args.device), strict=True)\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    before_acc_results.append(acc_test)\n",
    "\n",
    "                net_local_list[0].eval()\n",
    "                \n",
    "                label= [i for i in range(num_classes)]                    \n",
    "\n",
    "\n",
    "                ldr_test = DataLoader(dataset_test, batch_size=args.local_bs, shuffle=False)\n",
    "                class_sums = {i: None for i in label}\n",
    "                class_counts = {i: None for i in label}\n",
    "\n",
    "\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for idx, (images, labels) in enumerate(ldr_test):\n",
    "                        images, labels = images.to(args.device), labels.to(args.device)\n",
    "                        features = net_local_list[0].extract_features(images)\n",
    "\n",
    "                        for i in range(len(label)):\n",
    "                            class_mask = labels == label[i]  \n",
    "\n",
    "                            if class_mask.any():  # 클래스에 속하는 데이터가 있는 경우에만 해당\n",
    "                                class_features = features[class_mask]\n",
    "                                class_sum = class_features.sum(dim=0)\n",
    "                                count=class_features.shape[0]\n",
    "\n",
    "                                if class_sums[label[i]]== None and class_counts[label[i]] == None:\n",
    "                                    class_sums[label[i]]=class_sum\n",
    "                                    class_counts[label[i]]=count\n",
    "                                else:\n",
    "                                    class_sums[label[i]]+=class_sum\n",
    "                                    class_counts[label[i]]+=count\n",
    "\n",
    "                #Get the class-wise feature centroid                    \n",
    "                class_mean_dict={}\n",
    "                for key, value in class_sums.items():\n",
    "                    if key in class_counts:\n",
    "                        class_mean_dict[key] = value / class_counts[key]\n",
    "\n",
    "                print(class_mean_dict)\n",
    "                #Get a Inner product of each class's centroid vector\n",
    "\n",
    "                sorted_keys=sorted(class_mean_dict)\n",
    "                print(sorted_keys)\n",
    "\n",
    "                class_mean_lst=[]\n",
    "\n",
    "\n",
    "                for key in sorted_keys:\n",
    "                    class_mean_lst.append(class_mean_dict[key])\n",
    "\n",
    "                class_mean_lst=torch.stack(class_mean_lst)\n",
    "\n",
    "\n",
    "                print(class_mean_lst.shape)\n",
    "                print(class_mean_lst.transpose(0,1).shape)\n",
    "\n",
    "                #Get the each centroid vector's norm\n",
    "                print(\"-------------Norm of each centroid vectors --------------\")\n",
    "                print(torch.diagonal(torch.mm(class_mean_lst, class_mean_lst.transpose(0,1))))\n",
    "\n",
    "\n",
    "                #Get the cosine similarity result between eacn centroid vectors\n",
    "\n",
    "\n",
    "                normalized_class_mean_lst=nn.functional.normalize(class_mean_lst, p=2, dim=1)\n",
    "                print(\"-------------Cosine similarity of Each Centroid vectors--------------\")\n",
    "                print(torch.mm(normalized_class_mean_lst, normalized_class_mean_lst.transpose(0,1)).cpu().numpy())\n",
    "                    \n",
    "                    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7373da",
   "metadata": {},
   "source": [
    "# Cosine Similarity of Centroid Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 2D array로 변환\n",
    "cos_sim_np = torch.mm(normalized_class_mean_lst, normalized_class_mean_lst.transpose(0,1)).cpu().numpy()\n",
    "# heatmap 그리기\n",
    "plt.imshow(cos_sim_np, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cos_sim_np))\n",
    "\n",
    "print(cos_sim_np)\n",
    "\n",
    "print(cos_sim_np.sum(axis=1))\n",
    "\n",
    "print(cos_sim_np.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987d862",
   "metadata": {},
   "source": [
    "# Centroid-Feature Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad76dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.diagonal(torch.mm(normalized_class_mean_lst,normalized_classifier.transpose(0,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Centroid-Feature Alignment of each class\")\n",
    "print(torch.diagonal(torch.mm(normalized_class_mean_lst,normalized_classifier.transpose(0,1))))\n",
    "\n",
    "class_index=[i for i in range(num_classes)]\n",
    "\n",
    "plt.bar(class_index, torch.diagonal(torch.mm(normalized_class_mean_lst,normalized_classifier.transpose(0,1))).cpu().detach().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53a1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedfn",
   "language": "python",
   "name": "fedfn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
